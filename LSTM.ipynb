{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 12:38:53.733342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/brad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/brad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "data = data[['movie_name', 'synopsis', 'genre']]\n",
    "data['synopsis'] = data['synopsis'].apply(preprocess_text)\n",
    "\n",
    "# Preprocess the genres\n",
    "data['genre'] = data['genre'].apply(lambda x: x.split('|'))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe embeddings\n",
    "embeddings_index = {}\n",
    "path = '../glove.6B/glove.6B.100d.txt'\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Tokenize and pad the text sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['synopsis'])\n",
    "sequences = tokenizer.texts_to_sequences(data['synopsis'])\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(layers=256, dropout=0.2, lr=0.001):# Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=300, trainable=False))\n",
    "    model.add(LSTM(layers, dropout=dropout, recurrent_dropout=dropout))\n",
    "    model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, genres_encoded, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brad/Desktop/EECS 487/Final_Project/LSTM.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mload_weights(checkpoint_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m compile_model()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_model(model, X_train, y_train, X_val, y_val)\n",
      "\u001b[1;32m/Users/brad/Desktop/EECS 487/Final_Project/LSTM.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(checkpoint_path, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m[model_checkpoint, early_stopping])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brad/Desktop/EECS%20487/Final_Project/LSTM.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_path)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 942\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    943\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    761\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    765\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    768\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 171\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    303\u001b[0m         args,\n\u001b[1;32m    304\u001b[0m         kwargs,\n\u001b[1;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:667\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    664\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    665\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 667\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    668\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1189\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1190\u001b[0m       original_func,\n\u001b[1;32m   1191\u001b[0m       args,\n\u001b[1;32m   1192\u001b[0m       kwargs,\n\u001b[1;32m   1193\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1194\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1195\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1196\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1197\u001b[0m       ))\n\u001b[1;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/9p/n09d6qfj5d99hs5gvj02l5kr0000gn/T/__autograph_generated_fileac6z6o1x.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m     outputs,\n\u001b[1;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1273\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3695\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 3696\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/engine/training.py:1054\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1054\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m   1055\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/optimizers/optimizer.py:542\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \n\u001b[1;32m    524\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[39m      None\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/keras/optimizers/optimizer.py:275\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m callable(var_list):\n\u001b[1;32m    273\u001b[0m             var_list \u001b[39m=\u001b[39m var_list()\n\u001b[0;32m--> 275\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list)\n\u001b[1;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:394\u001b[0m, in \u001b[0;36m_WhileGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39m# We compute the gradient for the sub-graph between trainable ys and xs\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m# with non-None incoming gradients. We later pad the None's to the list of\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m# outputs.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m ys, xs, non_none_grads \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(y, x, grad) \u001b[39mfor\u001b[39;00m (y, x, grad) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    392\u001b[0m     body_graph\u001b[39m.\u001b[39moutputs, body_graph\u001b[39m.\u001b[39minputs, grads) \u001b[39mif\u001b[39;00m grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[0;32m--> 394\u001b[0m body_grad_graph, args \u001b[39m=\u001b[39m _create_grad_func(\n\u001b[1;32m    395\u001b[0m     ys, xs, non_none_grads, cond_graph, body_graph,\n\u001b[1;32m    396\u001b[0m     util\u001b[39m.\u001b[39;49munique_grad_fn_name(body_graph\u001b[39m.\u001b[39;49mname), op, maximum_iterations,\n\u001b[1;32m    397\u001b[0m     stateful_parallelism)\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m body_grad_graph\u001b[39m.\u001b[39mwhile_op_needs_rewrite:\n\u001b[1;32m    400\u001b[0m   \u001b[39m# Modify 'op' to output the intermediate accumulators needed by the grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m   \u001b[39m# function.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m   \u001b[39m# NOTE(skyewm): if there are any active sessions, this modification to `op`\u001b[39;00m\n\u001b[1;32m    403\u001b[0m   \u001b[39m# may make them unrunnable!\u001b[39;00m\n\u001b[1;32m    405\u001b[0m   cond_graph\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_rewritten\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:696\u001b[0m, in \u001b[0;36m_create_grad_func\u001b[0;34m(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations, stateful_parallelism)\u001b[0m\n\u001b[1;32m    693\u001b[0m args \u001b[39m=\u001b[39m [counter, maximum_iterations, total_iters] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(grads)\n\u001b[1;32m    694\u001b[0m \u001b[39m# Note: The returned function does not have `args` in the list of\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m# `external_captures`.\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m grad_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    697\u001b[0m     name,\n\u001b[1;32m    698\u001b[0m     \u001b[39mlambda\u001b[39;49;00m \u001b[39m*\u001b[39;49margs: _grad_fn(ys, xs, args, body_graph),\n\u001b[1;32m    699\u001b[0m     args, {},\n\u001b[1;32m    700\u001b[0m     func_graph\u001b[39m=\u001b[39;49m_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n\u001b[1;32m    701\u001b[0m                                        maximum_iterations, while_op,\n\u001b[1;32m    702\u001b[0m                                        body_graph_inputs, body_graph_outputs),\n\u001b[1;32m    703\u001b[0m     acd_record_initial_resource_uses\u001b[39m=\u001b[39;49mstateful_parallelism)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Update the list of outputs with tensors corresponding to the captured\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m# tensors. We capture 3 types of tensors when building the grad fn:\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39m# 1. Accumulators for forward graph intermediates which are not loop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39m# 2. Resources, which are output as is.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# 3. Forward graph loop invariants, which are output as is.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mfor\u001b[39;00m external_capture, internal_capture \u001b[39min\u001b[39;00m grad_func_graph\u001b[39m.\u001b[39mcaptures:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:698\u001b[0m, in \u001b[0;36m_create_grad_func.<locals>.<lambda>\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    693\u001b[0m args \u001b[39m=\u001b[39m [counter, maximum_iterations, total_iters] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(grads)\n\u001b[1;32m    694\u001b[0m \u001b[39m# Note: The returned function does not have `args` in the list of\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m# `external_captures`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m grad_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39mfunc_graph_from_py_func(\n\u001b[1;32m    697\u001b[0m     name,\n\u001b[0;32m--> 698\u001b[0m     \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs: _grad_fn(ys, xs, args, body_graph),\n\u001b[1;32m    699\u001b[0m     args, {},\n\u001b[1;32m    700\u001b[0m     func_graph\u001b[39m=\u001b[39m_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n\u001b[1;32m    701\u001b[0m                                        maximum_iterations, while_op,\n\u001b[1;32m    702\u001b[0m                                        body_graph_inputs, body_graph_outputs),\n\u001b[1;32m    703\u001b[0m     acd_record_initial_resource_uses\u001b[39m=\u001b[39mstateful_parallelism)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Update the list of outputs with tensors corresponding to the captured\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m# tensors. We capture 3 types of tensors when building the grad fn:\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39m# 1. Accumulators for forward graph intermediates which are not loop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39m# 2. Resources, which are output as is.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# 3. Forward graph loop invariants, which are output as is.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mfor\u001b[39;00m external_capture, internal_capture \u001b[39min\u001b[39;00m grad_func_graph\u001b[39m.\u001b[39mcaptures:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:754\u001b[0m, in \u001b[0;36m_grad_fn\u001b[0;34m(ys, xs, args, func_graph)\u001b[0m\n\u001b[1;32m    747\u001b[0m grad_ys \u001b[39m=\u001b[39m args[\u001b[39m3\u001b[39m:]\n\u001b[1;32m    749\u001b[0m \u001b[39m# Build the gradient graph. Note that this builds the gradient computation of\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39m# func_graph in the current graph, which requires capturing tensors from\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[39m# func_graph. The captured func_graph tensors are resolved to external tensors\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39m# after the forward While op has been rewritten in _resolve_grad_captures.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m# TODO(srbs): Mark GradientsHelper as public?\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m grad_outs \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39;49m_GradientsHelper(\n\u001b[1;32m    755\u001b[0m     ys, xs, grad_ys\u001b[39m=\u001b[39;49mgrad_ys, src_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    756\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzero\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    758\u001b[0m \u001b[39m# TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[39m# is a tf.StopGradient in the loop body.\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grad_outs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gradients_util.py:695\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[1;32m    691\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[1;32m    693\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[1;32m    694\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m    696\u001b[0m                              \u001b[39mlambda\u001b[39;49;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[1;32m    697\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m    701\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gradients_util.py:329\u001b[0m, in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    326\u001b[0m     xla_compile \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xla_compile:\n\u001b[0;32m--> 329\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn()  \u001b[39m# Exit early\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# If the gradients are supposed to be compiled separately, we give them a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# _XlaScope name that is based on the name_scope of the gradients.  Otherwise\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# they just inherit the existing _XlaScope name, which lets them be merged\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m# together with the non-gradient computation.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m xla_separate_compiled_gradients:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gradients_util.py:696\u001b[0m, in \u001b[0;36m_GradientsHelper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[1;32m    691\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[1;32m    693\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[1;32m    694\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 696\u001b[0m                              \u001b[39mlambda\u001b[39;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[1;32m    697\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m    701\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/list_ops.py:290\u001b[0m, in \u001b[0;36m_TensorListGetItemGrad\u001b[0;34m(op, ditem)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gradient for TensorListGetItem.\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m list_size \u001b[39m=\u001b[39m gen_list_ops\u001b[39m.\u001b[39mtensor_list_length(op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    289\u001b[0m list_grad \u001b[39m=\u001b[39m gen_list_ops\u001b[39m.\u001b[39mtensor_list_set_item(\n\u001b[0;32m--> 290\u001b[0m     gen_list_ops\u001b[39m.\u001b[39;49mtensor_list_reserve(\n\u001b[1;32m    291\u001b[0m         gen_list_ops\u001b[39m.\u001b[39;49mtensor_list_element_shape(op\u001b[39m.\u001b[39;49minputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    292\u001b[0m                                                shape_type\u001b[39m=\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mint32),\n\u001b[1;32m    293\u001b[0m         list_size, element_dtype\u001b[39m=\u001b[39;49mditem\u001b[39m.\u001b[39;49mdtype),\n\u001b[1;32m    294\u001b[0m     index\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m],\n\u001b[1;32m    295\u001b[0m     item\u001b[39m=\u001b[39mditem)\n\u001b[1;32m    296\u001b[0m index_grad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    297\u001b[0m element_shape_grad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gen_list_ops.py:886\u001b[0m, in \u001b[0;36mtensor_list_reserve\u001b[0;34m(element_shape, num_elements, element_dtype, name)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m element_dtype \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_type(element_dtype, \u001b[39m\"\u001b[39m\u001b[39melement_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 886\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m    887\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mTensorListReserve\u001b[39;49m\u001b[39m\"\u001b[39;49m, element_shape\u001b[39m=\u001b[39;49melement_shape,\n\u001b[1;32m    888\u001b[0m                            num_elements\u001b[39m=\u001b[39;49mnum_elements,\n\u001b[1;32m    889\u001b[0m                            element_dtype\u001b[39m=\u001b[39;49melement_dtype, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    890\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    790\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    791\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[1;32m    792\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    793\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    796\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m    797\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m    799\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:1045\u001b[0m, in \u001b[0;36m_WhileBodyGradFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m (op_type \u001b[39min\u001b[39;00m optimized_reduction_ops \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39moutput_all_intermediates() \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m     \u001b[39mall\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mgraph \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39mgraph_wrapped_for_higher_order_tape_gradients(\n\u001b[1;32m   1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph)):\n\u001b[1;32m   1035\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_op_to_forward_graph(\n\u001b[1;32m   1036\u001b[0m       op_type,\n\u001b[1;32m   1037\u001b[0m       inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1042\u001b[0m       op_def\u001b[39m=\u001b[39mop_def,\n\u001b[1;32m   1043\u001b[0m       compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m-> 1045\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_WhileBodyGradFuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(\n\u001b[1;32m   1046\u001b[0m     op_type,\n\u001b[1;32m   1047\u001b[0m     inputs,\n\u001b[1;32m   1048\u001b[0m     dtypes\u001b[39m=\u001b[39;49mdtypes,\n\u001b[1;32m   1049\u001b[0m     input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m   1050\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1051\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1052\u001b[0m     op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[1;32m   1053\u001b[0m     compute_device\u001b[39m=\u001b[39;49mcompute_device)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:705\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[39mif\u001b[39;00m ctxt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ctxt, \u001b[39m\"\u001b[39m\u001b[39mAddValue\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    704\u001b[0m     inp \u001b[39m=\u001b[39m ctxt\u001b[39m.\u001b[39mAddValue(inp)\n\u001b[0;32m--> 705\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcapture(inp)\n\u001b[1;32m    706\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m    707\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    708\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    709\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:772\u001b[0m, in \u001b[0;36mFuncGraph.capture\u001b[0;34m(self, tensor, name, shape)\u001b[0m\n\u001b[1;32m    762\u001b[0m       \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mInaccessibleTensorError(\n\u001b[1;32m    763\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m!r}\u001b[39;00m\u001b[39m is out of scope and cannot be used here. Use return \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mvalues, explicit Python locals or TensorFlow collections to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe tensor \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m!r}\u001b[39;00m\u001b[39m cannot be accessed from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, because \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    770\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mit was defined in \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39mgraph\u001b[39m}\u001b[39;00m\u001b[39m, which is out of scope.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    771\u001b[0m     inner_graph \u001b[39m=\u001b[39m inner_graph\u001b[39m.\u001b[39mouter_graph\n\u001b[0;32m--> 772\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_helper(tensor, name)\n\u001b[1;32m    773\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/while_v2.py:1218\u001b[0m, in \u001b[0;36m_WhileBodyGradFuncGraph._capture_helper\u001b[0;34m(self, tensor, name)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# Push the intermediate tensor to the tensor list. This captures\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# `tensor_list`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m-> 1218\u001b[0m   accumulator \u001b[39m=\u001b[39m list_ops\u001b[39m.\u001b[39;49mtensor_list_push_back(tensor_list, tensor)\n\u001b[1;32m   1219\u001b[0m \u001b[39m# Add the modified tensor list to the list of outputs. This output will be\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39m# all the accumulated values.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(accumulator)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/ops/gen_list_ops.py:762\u001b[0m, in \u001b[0;36mtensor_list_push_back\u001b[0;34m(input_handle, tensor, name)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m    763\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mTensorListPushBack\u001b[39;49m\u001b[39m\"\u001b[39;49m, input_handle\u001b[39m=\u001b[39;49minput_handle, tensor\u001b[39m=\u001b[39;49mtensor,\n\u001b[1;32m    764\u001b[0m                             name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    765\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    790\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    791\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[1;32m    792\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    793\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    796\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m    797\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m    799\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    705\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    706\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> 707\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    708\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    709\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3814\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 3814\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[1;32m   3815\u001b[0m       node_def,\n\u001b[1;32m   3816\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3817\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m   3818\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[1;32m   3819\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[1;32m   3820\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m   3821\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[1;32m   3822\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m   3823\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m   3824\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2112\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2109\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[1;32m   2111\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[0;32m-> 2112\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m   2113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_c_op(c_op\u001b[39m=\u001b[39mc_op, g\u001b[39m=\u001b[39mg)\n\u001b[1;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_op \u001b[39m=\u001b[39m original_op\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1954\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     pywrap_tf_session\u001b[39m.\u001b[39mTF_AddInputList(op_desc,\n\u001b[1;32m   1952\u001b[0m                                       [t\u001b[39m.\u001b[39m_as_tf_output() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m op_input])\n\u001b[1;32m   1953\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1954\u001b[0m     pywrap_tf_session\u001b[39m.\u001b[39;49mTF_AddInput(op_desc, op_input\u001b[39m.\u001b[39;49m_as_tf_output())\n\u001b[1;32m   1956\u001b[0m \u001b[39m# Add control inputs\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[39mfor\u001b[39;00m control_input \u001b[39min\u001b[39;00m control_inputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    checkpoint_path = \"models/LSTM_checkpoint.h5\"\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_val, y_val), callbacks=[model_checkpoint, early_stopping])\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "model = compile_model()\n",
    "train_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model()\n",
    "model.load_weights('models/LSTM_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(X, Y, k=5):\n",
    "    dropouts = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
    "    hidden = [32, 64, 128, 256, 512]\n",
    "    learning_rates = [0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "    high_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for d in dropouts:\n",
    "        for h in hidden:\n",
    "            for lr in learning_rates:\n",
    "                model = compile_model(h, d, lr)\n",
    "\n",
    "                kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    X_train, X_val = X[train_index], X[test_index]\n",
    "                    y_train, y_val = Y[train_index], Y[test_index]\n",
    "\n",
    "                    checkpoint_path = \"models/LSTM_checkpoint.h5\"\n",
    "                    model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "                    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n",
    "\n",
    "                    history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_val, y_val), callbacks=[model_checkpoint, early_stopping])\n",
    "                    best_val = max(history.history['val_accuracy'])\n",
    "                    if best_val > high_score:\n",
    "                        high_score = best_val\n",
    "                        best_params['dropout'] = d\n",
    "                        best_params['hidden layers'] = h\n",
    "                        best_params['learning rate'] = lr\n",
    "    print('Accuracy:', high_score)\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 22s 126ms/step\n",
      "Accuracy: 0.37407407407407406\n",
      "Precision: 0.3556678496445766\n",
      "Recall: 0.37177634037448415\n",
      "F1-score: 0.3521443048703147\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_pred_onehot = to_categorical(y_pred_classes, num_classes=10)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_onehot)\n",
    "precision = precision_score(y_test, y_pred_onehot, average='macro')\n",
    "recall = recall_score(y_test, y_pred_onehot, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_onehot, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: Super Me\n",
      "Actual genres: fantasy\n",
      "Predicted genres: mystery\n",
      "\n",
      "Movie: Behavioral Family Therapy for Serious Psychiatric Disorders\n",
      "Actual genres: family\n",
      "Predicted genres: adventure\n",
      "\n",
      "Movie: Blood Glacier\n",
      "Actual genres: scifi\n",
      "Predicted genres: horror\n",
      "\n",
      "Movie: Apat na anino\n",
      "Actual genres: action\n",
      "Predicted genres: romance\n",
      "\n",
      "Movie: Le dmon dans l'le\n",
      "Actual genres: horror\n",
      "Predicted genres: horror\n",
      "\n",
      "Movie: Hired\n",
      "Actual genres: crime\n",
      "Predicted genres: romance\n",
      "\n",
      "Movie: Until You See Me\n",
      "Actual genres: mystery\n",
      "Predicted genres: scifi\n",
      "\n",
      "Movie: Shamus\n",
      "Actual genres: mystery\n",
      "Predicted genres: mystery\n",
      "\n",
      "Movie: Crushed\n",
      "Actual genres: horror\n",
      "Predicted genres: thriller\n",
      "\n",
      "Movie: Vampires\n",
      "Actual genres: horror\n",
      "Predicted genres: fantasy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify misclassified instances\n",
    "misclassified = []\n",
    "for i in range(5400):\n",
    "    if (np.any(y_test[i] != y_pred_onehot[i])):\n",
    "        misclassified.append(i)\n",
    "misclassified = np.where(np.any(y_test != y_pred_onehot, axis=1))[0]\n",
    "\n",
    "# Analyze misclassified instances and suggest improvements\n",
    "for idx in misclassified[:10]:\n",
    "    print(f\"Movie: {data['movie_name'][idx]}\")\n",
    "    print(f\"Actual genres: {', '.join(mlb.classes_[y_pred_onehot[idx] == 1])}\")\n",
    "    print(f\"Predicted genres: {', '.join(mlb.classes_[y_pred_onehot[idx] == 1])}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
